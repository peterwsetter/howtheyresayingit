---
title: "Building a MonetDBlite Database with Tokenized Text"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

Last Updated: `r Sys.Date()`

In support of the larger goal of analyzing the sentiment of news articles, I wanted to begin by developing a work flow on a relatively easy dataset before developing a web-scraping script and applying these techniques to headlines and articles from various times.

For this project, I decided to use [MonetDBLite](https://www.monetdb.org/blog/monetdblite-r). Like SQLite, MonetDBLite is a file-based system that can be run from within another application. I've used `RSQLite` in the past, and I loved beginning able to quickly setup a database within R without having to manage a database server. `MonetDBLite` provides this functionality along with the features of a modern database, including proper `date` types. 

The [Github README](https://github.com/hannesmuehleisen/MonetDBLite-R) provides everything you need to know to get started.

```{r}
install.packages('MonetDBLite')

library(DBI)

dbdir <- "../data/bbcdb"

con <- dbConnect(MonetDBLite::MonetDBLite(), dbdir)
```

I decided to use the [BBC News Summary](https://www.kaggle.com/pariza/bbc-news-summary) dataset from Kaggle. The main advantage of this dataset is its uniform structure.

- Article text are stored in plain text files. No worries about formatting or extraneous material.
- Each articles contains the headline, the lead, and three or four paragraphs.
- Articles are organized into topic: business, entertainment, politics, sport, and tech. Each file has a number but not associated publication date.

Since I'm interested in looking at the sentiment of headlines and their headlines separately (and in comparison), I'll use two tables: `articles` and `article_content`. In order to exert more control over the table creation, I'll define the tables before populating them.

In the `articles` table, I'll utilitize the folder and file structure for the `article_id`. I'll also include the folder name as a separate column, `topic`. The last two columns will be the headline and lead.

```{r create_articles}
dbSendQuery(con,
              "CREATE TABLE articles (
              article_id varchar(20) PRIMARY KEY,
              topic varchar(10),
              headline text,
              lead text
              )")
```

The `article_content` table will use will have a separate row for each paragraph, identified by `paragraph_num`, and used with the `article_id` for the primary key. The `paragraph_text` is the third column. 

```{r create_article_content}
dbSendQuery(con,
            "CREATE TABLE article_content (
            article_id varchar(20),
            paragraph_num int,
            paragraph_text text,
            PRIMARY KEY(article_id, paragraph_num))
            ")
```

When working with multiple files that need to be processed in the same way, my general workflow is pipe `list.files` to `map`ping a processing function. If I'm using the data in session, I use `map_df`. If I'm writing each to a database, I include the `dbWriteTable` statement within the processing function.  

```{r define_process_file}
process_file <- function(file_name, db_con) {
  # file_name character string of file_path
  # db_con DBI connection object
  
  # readLines reads in a text file, breaking into a vector on '\n'
  # The second line removes empty strings caused by the double '\n' 
  # for paragrah breaks
  article <- readLines(file_name) %>% 
    .[. != '']
  
  # Files are separated by topic which is in the folder name
  topic <- str_extract(file_name, '(?<=News Articles/).*(?=/)')
  # Article number is the file name
  article_num <- str_extract(file_name, '\\d{3}')
  # Combine for unique id
  article_id <- paste0(topic, article_num)
  
  # Write article information
  data_frame(article_id = article_id,
             topic = topic,
             headline = article[1],
             lead = article[2]) %>% 
    dbWriteTable(db_con, 'articles', value = ., append = TRUE)
  
  # The paragraphs are identified number
  # Write to the database
  data_frame(paragraph_text = article[3:length(article)]) %>% 
    tibble::rowid_to_column('paragraph_num') %>% 
    mutate(article_id = article_id) %>% 
    select(article_id, paragraph_num, paragraph_text) %>% 
    dbWriteTable(db_con, 'article_content', value = ., append = TRUE)
}

```

Populating the database is now as simple as:

```{r populated_db}
list.files(path = '../../data/BBC News Summary/News Articles',
           full.names = TRUE, # Important for passing onto readLines
           recursive = TRUE # Include files in subfolders
           ) %>% 
  # Write data to database
  map(~process_file(.x, con))
```

```{r}

```

```{r shutdown}
dbDisconnect(con, shutdown=TRUE)

```